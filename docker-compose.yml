services:
  llama3:
      image: tolak/ollama-llama3
      ports:
        - 4434:11434
      container_name: ollama-llama3
      tty: true
      restart: always
      volumes:
        - ./ollama/ollama:/root/.ollama
      networks:
        - llama3-network
  signproxy:
    image: tolak/llama3-signproxy
    ports:
      - 4399:3000
    container_name: signproxy
    environment:
        - LLAMA3_ENDPOINT=http://host.docker.internal:4434/api/chat
    depends_on:
      - llama3
    networks:
      - llama3-network
    extra_hosts:
      - "host.docker.internal:host-gateway"

networks:
  llama3-network:
    external: false